{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "# refer: https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "d = {'text': ['when thieves broke into my house at night, and held my wife and me on gun point for at least ten minutes and took away a lot  of property.',\n",
    "                 'International sports events won by myfavourite national team or playerbrings me joy when india won the world cup cricket match',\n",
    "                 'I was sitting in a restaurant with friends they asked me something which they thoughtI should know Actually I know it but atthat time I was not able to remember it']}\n",
    "d_ex = pd.DataFrame(data=d)\n",
    "d_ex['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                  text\n",
      "0                           when thieves broke into my house at night, and held my wife and me on gun point for at least ten minutes and took away a lot  of property.\n",
      "1                                       International sports events won by myfavourite national team or playerbrings me joy when india won the world cup cricket match\n",
      "2  I was sitting in a restaurant with friends they asked me something which they thoughtI should know Actually I know it but atthat time I was not able to remember it\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 500):\n",
    "    print(d_ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/ilab/users/kc1026/Documents/cs543/sentiment140_clean.csv\", sep=',', header=0)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(index=str, columns={\"Unnamed: 0\": \"index\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import re\n",
    "stopWords = set(STOPWORDS)\n",
    "lmtzr = nltk.WordNetLemmatizer().lemmatize\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def normalize_text(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    cleaned_token_text = []\n",
    "    for tt in token_text:\n",
    "        if tt in stopWords or tt == '' or len(tt) < 2 or tt : continue    \n",
    "        cleaned_token_text.append(tt)\n",
    "    \n",
    "    word_pos = nltk.pos_tag(cleaned_token_text)\n",
    "    lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n",
    "    \n",
    "    return [x.lower() for x in lemm_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df['text'].map(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "# count = 0\n",
    "# for k, v in dictionary.iteritems():\n",
    "#     print(k, v)\n",
    "#     count += 1\n",
    "#     if count > 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('lda_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(keep_n=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=75, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.218*\"yes\" + 0.140*\"ok\" + 0.042*\"sister\" + 0.039*\"nite\" + 0.035*\"text\" + 0.032*\"appreciate\" + 0.029*\"thought\" + 0.025*\"aw\" + 0.025*\"lol\" + 0.023*\"project\"\n",
      "Topic: 1 \n",
      "Words: 0.104*\"damn\" + 0.071*\"chat\" + 0.057*\"awake\" + 0.052*\"absolutely\" + 0.050*\"wine\" + 0.049*\"stupid\" + 0.045*\"bbq\" + 0.044*\"realize\" + 0.038*\"invite\" + 0.038*\"freak\"\n",
      "Topic: 2 \n",
      "Words: 0.188*\"take\" + 0.126*\"rock\" + 0.090*\"walk\" + 0.080*\"ill\" + 0.079*\"super\" + 0.033*\"foot\" + 0.032*\"tune\" + 0.030*\"boyfriend\" + 0.027*\"street\" + 0.021*\"doubt\"\n",
      "Topic: 3 \n",
      "Words: 0.249*\"twitter\" + 0.095*\"finally\" + 0.067*\"funny\" + 0.048*\"real\" + 0.044*\"goodnight\" + 0.041*\"give\" + 0.029*\"line\" + 0.027*\"online\" + 0.026*\"city\" + 0.019*\"new\"\n",
      "Topic: 4 \n",
      "Words: 0.128*\"add\" + 0.122*\"later\" + 0.087*\"picture\" + 0.080*\"plan\" + 0.045*\"date\" + 0.038*\"cuz\" + 0.030*\"hold\" + 0.030*\"congratulation\" + 0.028*\"alright\" + 0.028*\"ipod\"\n",
      "Topic: 5 \n",
      "Words: 0.134*\"talk\" + 0.132*\"hour\" + 0.061*\"till\" + 0.055*\"drive\" + 0.047*\"join\" + 0.045*\"pm\" + 0.034*\"bless\" + 0.032*\"moment\" + 0.031*\"ppl\" + 0.026*\"xoxo\"\n",
      "Topic: 6 \n",
      "Words: 0.166*\"family\" + 0.146*\"exam\" + 0.106*\"study\" + 0.099*\"june\" + 0.068*\"feeling\" + 0.041*\"mall\" + 0.039*\"tweeps\" + 0.038*\"go\" + 0.034*\"airport\" + 0.026*\"today\"\n",
      "Topic: 7 \n",
      "Words: 0.154*\"house\" + 0.145*\"sweet\" + 0.063*\"pick\" + 0.057*\"award\" + 0.048*\"chocolate\" + 0.042*\"save\" + 0.033*\"small\" + 0.029*\"business\" + 0.023*\"draw\" + 0.022*\"pre\"\n",
      "Topic: 8 \n",
      "Words: 0.253*\"come\" + 0.240*\"look\" + 0.154*\"awesome\" + 0.051*\"sun\" + 0.047*\"forward\" + 0.020*\"good\" + 0.016*\"episode\" + 0.014*\"great\" + 0.013*\"son\" + 0.013*\"road\"\n",
      "Topic: 9 \n",
      "Words: 0.283*\"sleep\" + 0.119*\"hot\" + 0.114*\"dinner\" + 0.068*\"lose\" + 0.047*\"bout\" + 0.037*\"go\" + 0.032*\"note\" + 0.029*\"need\" + 0.027*\"night\" + 0.026*\"daughter\"\n",
      "Topic: 10 \n",
      "Words: 0.350*\"tomorrow\" + 0.190*\"school\" + 0.079*\"wake\" + 0.071*\"go\" + 0.063*\"care\" + 0.051*\"high\" + 0.026*\"college\" + 0.025*\"today\" + 0.019*\"ll\" + 0.018*\"spending\"\n",
      "Topic: 11 \n",
      "Words: 0.289*\"try\" + 0.177*\"follower\" + 0.074*\"crazy\" + 0.067*\"laugh\" + 0.042*\"cook\" + 0.040*\"luv\" + 0.027*\"new\" + 0.024*\"background\" + 0.023*\"ll\" + 0.020*\"advice\"\n",
      "Topic: 12 \n",
      "Words: 0.186*\"thats\" + 0.122*\"btw\" + 0.087*\"open\" + 0.078*\"suck\" + 0.055*\"beer\" + 0.045*\"card\" + 0.042*\"begin\" + 0.038*\"stand\" + 0.031*\"review\" + 0.029*\"felt\"\n",
      "Topic: 13 \n",
      "Words: 0.268*\"yay\" + 0.128*\"send\" + 0.081*\"call\" + 0.066*\"email\" + 0.048*\"message\" + 0.031*\"anymore\" + 0.026*\"yr\" + 0.022*\"mess\" + 0.022*\"folk\" + 0.021*\"nail\"\n",
      "Topic: 14 \n",
      "Words: 0.227*\"na\" + 0.209*\"gon\" + 0.079*\"friday\" + 0.042*\"hell\" + 0.037*\"moon\" + 0.032*\"tuesday\" + 0.027*\"mood\" + 0.027*\"new\" + 0.023*\"thursday\" + 0.015*\"trailer\"\n",
      "Topic: 15 \n",
      "Words: 0.265*\"way\" + 0.077*\"sit\" + 0.067*\"hopefully\" + 0.060*\"room\" + 0.052*\"facebook\" + 0.043*\"band\" + 0.042*\"sunny\" + 0.041*\"couple\" + 0.026*\"pizza\" + 0.023*\"home\"\n",
      "Topic: 16 \n",
      "Words: 0.191*\"morning\" + 0.184*\"good\" + 0.179*\"night\" + 0.089*\"bed\" + 0.068*\"ya\" + 0.045*\"early\" + 0.040*\"go\" + 0.026*\"time\" + 0.025*\"saturday\" + 0.017*\"shopping\"\n",
      "Topic: 17 \n",
      "Words: 0.287*\"haha\" + 0.167*\"yeah\" + 0.078*\"live\" + 0.035*\"visit\" + 0.032*\"kinda\" + 0.028*\"wear\" + 0.025*\"lol\" + 0.025*\"oh\" + 0.025*\"cold\" + 0.020*\"website\"\n",
      "Topic: 18 \n",
      "Words: 0.100*\"sorry\" + 0.099*\"game\" + 0.095*\"hear\" + 0.079*\"kid\" + 0.072*\"rain\" + 0.048*\"trip\" + 0.044*\"xd\" + 0.034*\"season\" + 0.030*\"woke\" + 0.029*\"shirt\"\n",
      "Topic: 19 \n",
      "Words: 0.148*\"phone\" + 0.082*\"learn\" + 0.072*\"final\" + 0.060*\"wrong\" + 0.053*\"fb\" + 0.050*\"london\" + 0.047*\"die\" + 0.043*\"whats\" + 0.036*\"hungry\" + 0.034*\"hun\"\n",
      "Topic: 20 \n",
      "Words: 0.538*\"work\" + 0.089*\"run\" + 0.032*\"gym\" + 0.031*\"today\" + 0.029*\"lakers\" + 0.022*\"go\" + 0.021*\"tired\" + 0.020*\"time\" + 0.020*\"taylor\" + 0.019*\"home\"\n",
      "Topic: 21 \n",
      "Words: 0.112*\"pay\" + 0.087*\"coffee\" + 0.079*\"food\" + 0.068*\"album\" + 0.050*\"star\" + 0.041*\"heard\" + 0.033*\"soooo\" + 0.027*\"good\" + 0.024*\"taste\" + 0.022*\"boo\"\n",
      "Topic: 22 \n",
      "Words: 0.227*\"enjoy\" + 0.090*\"tire\" + 0.085*\"weather\" + 0.077*\"amazing\" + 0.064*\"sooo\" + 0.034*\"fantastic\" + 0.034*\"hahah\" + 0.029*\"tan\" + 0.029*\"roll\" + 0.025*\"expect\"\n",
      "Topic: 23 \n",
      "Words: 0.244*\"tweet\" + 0.118*\"finish\" + 0.108*\"beautiful\" + 0.079*\"late\" + 0.056*\"turn\" + 0.042*\"sing\" + 0.038*\"dear\" + 0.024*\"type\" + 0.024*\"yo\" + 0.023*\"stick\"\n",
      "Topic: 24 \n",
      "Words: 0.323*\"thank\" + 0.198*\"girl\" + 0.139*\"head\" + 0.048*\"story\" + 0.034*\"hurt\" + 0.025*\"past\" + 0.020*\"magic\" + 0.013*\"code\" + 0.013*\"goodness\" + 0.010*\"boring\"\n",
      "Topic: 25 \n",
      "Words: 0.145*\"job\" + 0.121*\"stay\" + 0.078*\"lady\" + 0.057*\"question\" + 0.050*\"weird\" + 0.044*\"cousin\" + 0.041*\"mail\" + 0.040*\"suppose\" + 0.037*\"good\" + 0.027*\"offer\"\n",
      "Topic: 26 \n",
      "Words: 0.161*\"movie\" + 0.126*\"eat\" + 0.089*\"see\" + 0.086*\"god\" + 0.059*\"drink\" + 0.046*\"hit\" + 0.038*\"problem\" + 0.030*\"miley\" + 0.029*\"cake\" + 0.022*\"good\"\n",
      "Topic: 27 \n",
      "Words: 0.259*\"friend\" + 0.228*\"best\" + 0.112*\"th\" + 0.081*\"lovely\" + 0.046*\"heart\" + 0.019*\"evening\" + 0.018*\"good\" + 0.015*\"mmmm\" + 0.014*\"rid\" + 0.012*\"gr\"\n",
      "Topic: 28 \n",
      "Words: 0.267*\"miss\" + 0.114*\"write\" + 0.102*\"boy\" + 0.088*\"far\" + 0.048*\"fly\" + 0.030*\"wednesday\" + 0.029*\"event\" + 0.027*\"paper\" + 0.026*\"future\" + 0.020*\"sir\"\n",
      "Topic: 29 \n",
      "Words: 0.258*\"world\" + 0.113*\"believe\" + 0.063*\"blue\" + 0.061*\"internet\" + 0.045*\"huh\" + 0.041*\"interesting\" + 0.038*\"positive\" + 0.037*\"p\" + 0.034*\"sky\" + 0.030*\"fry\"\n",
      "Topic: 30 \n",
      "Words: 0.067*\"dad\" + 0.063*\"shop\" + 0.059*\"ice\" + 0.046*\"cream\" + 0.043*\"yummy\" + 0.041*\"bye\" + 0.038*\"short\" + 0.036*\"beat\" + 0.036*\"light\" + 0.033*\"mum\"\n",
      "Topic: 31 \n",
      "Words: 0.141*\"wan\" + 0.138*\"na\" + 0.122*\"sunday\" + 0.089*\"car\" + 0.088*\"favorite\" + 0.050*\"em\" + 0.034*\"voice\" + 0.027*\"fit\" + 0.024*\"excellent\" + 0.024*\"def\"\n",
      "Topic: 32 \n",
      "Words: 0.722*\"love\" + 0.108*\"make\" + 0.030*\"sad\" + 0.013*\"wont\" + 0.013*\"watchin\" + 0.010*\"new\" + 0.009*\"personal\" + 0.009*\"imagine\" + 0.008*\"choose\" + 0.007*\"lol\"\n",
      "Topic: 33 \n",
      "Words: 0.206*\"ready\" + 0.181*\"pic\" + 0.055*\"dress\" + 0.052*\"yep\" + 0.046*\"seriously\" + 0.045*\"safe\" + 0.040*\"fast\" + 0.038*\"radio\" + 0.026*\"dead\" + 0.022*\"treat\"\n",
      "Topic: 34 \n",
      "Words: 0.496*\"watch\" + 0.084*\"hello\" + 0.061*\"tv\" + 0.029*\"mother\" + 0.023*\"ma\" + 0.022*\"arrive\" + 0.020*\"paint\" + 0.018*\"present\" + 0.014*\"tweeter\" + 0.014*\"wedding\"\n",
      "Topic: 35 \n",
      "Words: 0.541*\"like\" + 0.182*\"feel\" + 0.039*\"good\" + 0.036*\"fuck\" + 0.022*\"ride\" + 0.015*\"lol\" + 0.013*\"remind\" + 0.012*\"crap\" + 0.011*\"today\" + 0.010*\"think\"\n",
      "Topic: 36 \n",
      "Words: 0.598*\"get\" + 0.061*\"ta\" + 0.033*\"clean\" + 0.033*\"st\" + 0.032*\"home\" + 0.031*\"ve\" + 0.025*\"goin\" + 0.020*\"today\" + 0.013*\"new\" + 0.013*\"demi\"\n",
      "Topic: 37 \n",
      "Words: 0.211*\"have\" + 0.113*\"hair\" + 0.053*\"xxx\" + 0.050*\"cut\" + 0.042*\"babe\" + 0.038*\"chicken\" + 0.038*\"ive\" + 0.030*\"time\" + 0.025*\"bath\" + 0.024*\"experience\"\n",
      "Topic: 38 \n",
      "Words: 0.134*\"saw\" + 0.109*\"mom\" + 0.051*\"tea\" + 0.047*\"lil\" + 0.046*\"til\" + 0.033*\"even\" + 0.032*\"exactly\" + 0.030*\"box\" + 0.029*\"art\" + 0.028*\"mmm\"\n",
      "Topic: 39 \n",
      "Words: 0.158*\"summer\" + 0.140*\"baby\" + 0.093*\"brother\" + 0.060*\"news\" + 0.045*\"special\" + 0.039*\"google\" + 0.033*\"lay\" + 0.026*\"group\" + 0.024*\"blood\" + 0.023*\"extra\"\n",
      "Topic: 40 \n",
      "Words: 0.179*\"maybe\" + 0.151*\"dont\" + 0.100*\"hard\" + 0.056*\"think\" + 0.046*\"hmm\" + 0.040*\"dm\" + 0.031*\"design\" + 0.027*\"pink\" + 0.027*\"ll\" + 0.026*\"hill\"\n",
      "Topic: 41 \n",
      "Words: 0.160*\"lunch\" + 0.121*\"bring\" + 0.102*\"ah\" + 0.095*\"site\" + 0.054*\"interview\" + 0.050*\"mention\" + 0.045*\"hilarious\" + 0.030*\"hannah\" + 0.029*\"doin\" + 0.025*\"time\"\n",
      "Topic: 42 \n",
      "Words: 0.385*\"know\" + 0.141*\"let\" + 0.033*\"money\" + 0.032*\"perfect\" + 0.029*\"list\" + 0.026*\"close\" + 0.024*\"ll\" + 0.022*\"lol\" + 0.018*\"mac\" + 0.017*\"think\"\n",
      "Topic: 43 \n",
      "Words: 0.204*\"hi\" + 0.102*\"congrats\" + 0.096*\"beach\" + 0.060*\"sick\" + 0.057*\"ticket\" + 0.038*\"vacation\" + 0.032*\"glass\" + 0.031*\"round\" + 0.029*\"pop\" + 0.027*\"quick\"\n",
      "Topic: 44 \n",
      "Words: 0.210*\"week\" + 0.088*\"book\" + 0.057*\"class\" + 0.053*\"haven\" + 0.053*\"probably\" + 0.052*\"blog\" + 0.036*\"page\" + 0.036*\"comment\" + 0.035*\"ago\" + 0.031*\"reason\"\n",
      "Topic: 45 \n",
      "Words: 0.133*\"buy\" + 0.117*\"stuff\" + 0.067*\"busy\" + 0.058*\"worry\" + 0.054*\"iphone\" + 0.049*\"new\" + 0.036*\"black\" + 0.033*\"account\" + 0.033*\"decide\" + 0.024*\"forever\"\n",
      "Topic: 46 \n",
      "Words: 0.254*\"play\" + 0.152*\"mean\" + 0.071*\"face\" + 0.046*\"bet\" + 0.041*\"kick\" + 0.035*\"ugh\" + 0.031*\"chillin\" + 0.029*\"window\" + 0.029*\"shout\" + 0.027*\"fail\"\n",
      "Topic: 47 \n",
      "Words: 0.255*\"happy\" + 0.223*\"nice\" + 0.106*\"birthday\" + 0.099*\"help\" + 0.056*\"use\" + 0.028*\"fall\" + 0.016*\"hmmm\" + 0.015*\"release\" + 0.015*\"today\" + 0.013*\"asleep\"\n",
      "Topic: 48 \n",
      "Words: 0.264*\"hey\" + 0.071*\"idea\" + 0.065*\"course\" + 0.053*\"reply\" + 0.041*\"hand\" + 0.034*\"wasn\" + 0.034*\"able\" + 0.033*\"fix\" + 0.031*\"da\" + 0.029*\"tom\"\n",
      "Topic: 49 \n",
      "Words: 0.118*\"man\" + 0.095*\"omg\" + 0.087*\"ha\" + 0.082*\"fan\" + 0.056*\"wonderful\" + 0.049*\"afternoon\" + 0.048*\"concert\" + 0.034*\"team\" + 0.032*\"meeting\" + 0.024*\"air\"\n",
      "Topic: 50 \n",
      "Words: 0.252*\"right\" + 0.178*\"sure\" + 0.152*\"wish\" + 0.101*\"free\" + 0.031*\"answer\" + 0.027*\"pretty\" + 0.025*\"ll\" + 0.022*\"flight\" + 0.019*\"lol\" + 0.016*\"space\"\n",
      "Topic: 51 \n",
      "Words: 0.143*\"bad\" + 0.132*\"welcome\" + 0.079*\"hehe\" + 0.077*\"dream\" + 0.044*\"hop\" + 0.041*\"point\" + 0.040*\"support\" + 0.034*\"excited\" + 0.032*\"followfriday\" + 0.030*\"record\"\n",
      "Topic: 52 \n",
      "Words: 0.153*\"year\" + 0.130*\"old\" + 0.115*\"luck\" + 0.099*\"good\" + 0.041*\"mr\" + 0.032*\"nd\" + 0.032*\"july\" + 0.028*\"different\" + 0.026*\"count\" + 0.021*\"ahead\"\n",
      "Topic: 53 \n",
      "Words: 0.274*\"amaze\" + 0.148*\"hate\" + 0.074*\"ahh\" + 0.072*\"wed\" + 0.056*\"shot\" + 0.055*\"david\" + 0.044*\"delicious\" + 0.034*\"lake\" + 0.022*\"princess\" + 0.020*\"title\"\n",
      "Topic: 54 \n",
      "Words: 0.170*\"music\" + 0.119*\"place\" + 0.112*\"smile\" + 0.083*\"lmao\" + 0.080*\"shower\" + 0.075*\"jonas\" + 0.048*\"download\" + 0.038*\"ahhh\" + 0.029*\"mcfly\" + 0.021*\"brazil\"\n",
      "Topic: 55 \n",
      "Words: 0.315*\"follow\" + 0.075*\"remember\" + 0.066*\"link\" + 0.038*\"english\" + 0.034*\"chance\" + 0.030*\"laptop\" + 0.025*\"service\" + 0.025*\"smell\" + 0.024*\"homework\" + 0.022*\"copy\"\n",
      "Topic: 56 \n",
      "Words: 0.197*\"song\" + 0.125*\"cute\" + 0.096*\"train\" + 0.051*\"eye\" + 0.049*\"la\" + 0.042*\"cd\" + 0.041*\"test\" + 0.035*\"new\" + 0.031*\"shall\" + 0.027*\"warm\"\n",
      "Topic: 57 \n",
      "Words: 0.106*\"win\" + 0.105*\"hang\" + 0.071*\"set\" + 0.068*\"half\" + 0.062*\"relax\" + 0.059*\"green\" + 0.058*\"red\" + 0.046*\"didnt\" + 0.039*\"sexy\" + 0.037*\"sort\"\n",
      "Topic: 58 \n",
      "Words: 0.176*\"cool\" + 0.135*\"ur\" + 0.093*\"post\" + 0.072*\"update\" + 0.064*\"monday\" + 0.049*\"fine\" + 0.031*\"myspace\" + 0.028*\"sunshine\" + 0.026*\"min\" + 0.020*\"especially\"\n",
      "Topic: 59 \n",
      "Words: 0.229*\"life\" + 0.153*\"party\" + 0.081*\"photo\" + 0.078*\"aww\" + 0.048*\"as\" + 0.033*\"understand\" + 0.029*\"nick\" + 0.026*\"tip\" + 0.023*\"gift\" + 0.021*\"clothes\"\n",
      "Topic: 60 \n",
      "Words: 0.204*\"weekend\" + 0.081*\"vip\" + 0.077*\"bore\" + 0.074*\"mind\" + 0.067*\"catch\" + 0.045*\"church\" + 0.043*\"cheer\" + 0.035*\"number\" + 0.035*\"hangover\" + 0.030*\"great\"\n",
      "Topic: 61 \n",
      "Words: 0.133*\"end\" + 0.107*\"totally\" + 0.093*\"happen\" + 0.081*\"agree\" + 0.058*\"show\" + 0.035*\"ate\" + 0.030*\"practice\" + 0.029*\"ohh\" + 0.028*\"thing\" + 0.027*\"put\"\n",
      "Topic: 62 \n",
      "Words: 0.135*\"away\" + 0.082*\"person\" + 0.078*\"soo\" + 0.077*\"hug\" + 0.071*\"office\" + 0.044*\"swim\" + 0.044*\"everybody\" + 0.039*\"pass\" + 0.036*\"thanx\" + 0.028*\"click\"\n",
      "Topic: 63 \n",
      "Words: 0.301*\"fun\" + 0.135*\"sound\" + 0.059*\"vote\" + 0.049*\"forget\" + 0.033*\"figure\" + 0.031*\"good\" + 0.030*\"water\" + 0.029*\"town\" + 0.023*\"lot\" + 0.021*\"deal\"\n",
      "Topic: 64 \n",
      "Words: 0.227*\"soon\" + 0.226*\"well\" + 0.067*\"breakfast\" + 0.055*\"shit\" + 0.046*\"worth\" + 0.045*\"move\" + 0.043*\"pack\" + 0.041*\"ll\" + 0.040*\"hope\" + 0.033*\"yum\"\n",
      "Topic: 65 \n",
      "Words: 0.380*\"wait\" + 0.058*\"minute\" + 0.042*\"yea\" + 0.036*\"nap\" + 0.033*\"sims\" + 0.025*\"sell\" + 0.025*\"finger\" + 0.023*\"cross\" + 0.023*\"guitar\" + 0.020*\"web\"\n",
      "Topic: 66 \n",
      "Words: 0.358*\"im\" + 0.134*\"glad\" + 0.124*\"read\" + 0.059*\"lol\" + 0.030*\"chill\" + 0.028*\"go\" + 0.028*\"bday\" + 0.019*\"mad\" + 0.018*\"company\" + 0.017*\"sleepy\"\n",
      "Topic: 67 \n",
      "Words: 0.173*\"meet\" + 0.131*\"ask\" + 0.065*\"second\" + 0.056*\"keep\" + 0.039*\"age\" + 0.038*\"officially\" + 0.036*\"load\" + 0.034*\"travel\" + 0.034*\"bag\" + 0.032*\"lie\"\n",
      "Topic: 68 \n",
      "Words: 0.566*\"want\" + 0.065*\"proud\" + 0.054*\"spend\" + 0.052*\"twilight\" + 0.030*\"dvd\" + 0.030*\"fav\" + 0.022*\"ton\" + 0.022*\"board\" + 0.021*\"jam\" + 0.016*\"floor\"\n",
      "Topic: 69 \n",
      "Words: 0.207*\"people\" + 0.072*\"word\" + 0.060*\"rest\" + 0.058*\"kind\" + 0.044*\"thx\" + 0.038*\"sign\" + 0.037*\"park\" + 0.037*\"cat\" + 0.031*\"holiday\" + 0.025*\"lazy\"\n",
      "Topic: 70 \n",
      "Words: 0.155*\"guy\" + 0.121*\"com\" + 0.111*\"listen\" + 0.086*\"wow\" + 0.085*\"www\" + 0.036*\"dog\" + 0.031*\"lucky\" + 0.031*\"awww\" + 0.029*\"youtube\" + 0.027*\"tour\"\n",
      "Topic: 71 \n",
      "Words: 0.322*\"thanks\" + 0.070*\"excite\" + 0.053*\"video\" + 0.052*\"bit\" + 0.039*\"stop\" + 0.036*\"change\" + 0.030*\"dance\" + 0.025*\"share\" + 0.023*\"easy\" + 0.021*\"order\"\n",
      "Topic: 72 \n",
      "Words: 0.198*\"tell\" + 0.125*\"hahaha\" + 0.084*\"true\" + 0.066*\"wonder\" + 0.059*\"definitely\" + 0.029*\"lol\" + 0.028*\"say\" + 0.025*\"xo\" + 0.022*\"jb\" + 0.019*\"studio\"\n",
      "Topic: 73 \n",
      "Words: 0.494*\"day\" + 0.153*\"hope\" + 0.051*\"good\" + 0.048*\"great\" + 0.045*\"today\" + 0.040*\"okay\" + 0.019*\"go\" + 0.010*\"start\" + 0.009*\"pray\" + 0.008*\"storm\"\n",
      "Topic: 74 \n",
      "Words: 0.208*\"long\" + 0.197*\"check\" + 0.094*\"xx\" + 0.084*\"month\" + 0.065*\"dude\" + 0.063*\"time\" + 0.037*\"gorgeous\" + 0.022*\"history\" + 0.015*\"pls\" + 0.013*\"jon\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('know', 0.3915512), ('let', 0.14674239), ('god', 0.07042134), ('idea', 0.050161228), ('happen', 0.04144807)]\n",
      "[('think', 0.47267756), ('lunch', 0.074404486), ('news', 0.040900342), ('da', 0.027649488), ('awake', 0.025618644)]\n",
      "[('old', 0.16212055), ('sorry', 0.15302135), ('hear', 0.14452542), ('remember', 0.08374428), ('lmao', 0.07042261)]\n",
      "[('win', 0.16647513), ('train', 0.09225803), ('place', 0.078995794), ('plan', 0.07622332), ('probably', 0.057463672)]\n",
      "[('read', 0.23063527), ('hard', 0.11917932), ('sit', 0.1178597), ('drive', 0.09761256), ('lady', 0.08243821)]\n",
      "[('sure', 0.15412599), ('summer', 0.10685616), ('finish', 0.10630787), ('ask', 0.068303294), ('fine', 0.049663723)]\n",
      "[('need', 0.34333673), ('vip', 0.06955794), ('trip', 0.056927953), ('story', 0.04533144), ('cover', 0.026527233)]\n"
     ]
    }
   ],
   "source": [
    "unseen_document='lolol'\n",
    "doc_bow = dictionary.doc2bow(normalize_text(unseen_document))\n",
    "doc_lda = lda_model[doc_bow]\n",
    "for index, score in sorted(doc_lda, key=lambda tup: -1*tup[1]):\n",
    "    print(lda_model.show_topic(index, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug_words_list = []\n",
    "for text in d_ex['text']:\n",
    "    doc_bow = dictionary.doc2bow(normalize_text(text))\n",
    "    doc_lda = lda_model[doc_bow]\n",
    "    temp = []\n",
    "    for index, score in sorted(doc_lda, key=lambda tup: -1*tup[1]):\n",
    "        for word, score in lda_model.show_topic(index, 5):\n",
    "            temp.append(word)\n",
    "    sug_words_list.append(temp)\n",
    "\n",
    "sug_words = pd.Series(sug_words_list)\n",
    "d_ex['suggested_words'] = sug_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec_path = \"/ilab/users/kc1026/Documents/cs543/GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 175303 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 150 # max number of words in a comment to use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     7480\n",
       "label    7480\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"/ilab/users/kc1026/Documents/cs543/emoji.csv\", sep=',', header=0)\n",
    "train, test = train_test_split(df2, test_size=0.1)\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/lib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(\"[^a-zA-Z']\", ' ', x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub(\"[^a-zA-Z']\", ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8506 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(train[\"text\"].tolist())\n",
    "x_training_sequences = tokenizer.texts_to_sequences(train[\"text\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "# set max length of sequences, now all data has the same length of 300\n",
    "train_cnn_data = pad_sequences(x_training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "num_words = len(train_word_index) + 1\n",
    "train_embedding_weights = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    if word in word2vec:\n",
    "        train_embedding_weights[index,:] = word2vec[word]     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test[\"text\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = test_cnn_data\n",
    "y_test = np_utils.to_categorical(test['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_train = np_utils.to_categorical(train['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 150, 300)     2552100     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 149, 100)     60100       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 148, 100)     90100       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 147, 100)     120100      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 100)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 100)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 100)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 300)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          77056       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 7)            1799        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 7)            0           dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,901,255\n",
      "Trainable params: 2,901,255\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# word2vec + CNN\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "\n",
    "#training params\n",
    "batch_size = 128 \n",
    "num_epochs = 6 \n",
    "drop_rate = 0.2\n",
    "\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_DIM, weights=[train_embedding_weights], input_length=MAX_SEQUENCE_LENGTH, trainable=True)(sequence_input)\n",
    "conv_0 = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(embedding_layer)\n",
    "maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "conv_1 = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(embedding_layer)\n",
    "maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "conv_2 = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(embedding_layer)\n",
    "maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "\n",
    "\n",
    "merged = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(drop_rate)(merged)\n",
    "merged = Dense(7)(merged)\n",
    "output = Activation('softmax')(merged)\n",
    "model = Model(inputs=[sequence_input], outputs=[output])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6058 samples, validate on 674 samples\n",
      "Epoch 1/6\n",
      "6058/6058 [==============================] - 13s 2ms/step - loss: 1.6299 - acc: 0.3977 - val_loss: 1.3200 - val_acc: 0.5252\n",
      "Epoch 2/6\n",
      "6058/6058 [==============================] - 11s 2ms/step - loss: 1.0363 - acc: 0.6459 - val_loss: 1.1376 - val_acc: 0.6083\n",
      "Epoch 3/6\n",
      "6058/6058 [==============================] - 11s 2ms/step - loss: 0.7583 - acc: 0.7451 - val_loss: 1.2125 - val_acc: 0.5772\n",
      "Epoch 4/6\n",
      "6058/6058 [==============================] - 10s 2ms/step - loss: 0.5285 - acc: 0.8369 - val_loss: 1.0831 - val_acc: 0.6231\n",
      "Epoch 5/6\n",
      "6058/6058 [==============================] - 10s 2ms/step - loss: 0.3449 - acc: 0.9019 - val_loss: 1.2831 - val_acc: 0.6068\n",
      "Epoch 6/6\n",
      "6058/6058 [==============================] - 10s 2ms/step - loss: 0.2070 - acc: 0.9454 - val_loss: 1.1318 - val_acc: 0.6513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1b7b6be2e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "3/3 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(pad_sequences(tokenizer.texts_to_sequences(d_ex[\"text\"].tolist()), maxlen=MAX_SEQUENCE_LENGTH)\n",
    ", batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_top1 = []\n",
    "for i in range(0, len(y_predict)):\n",
    "    max_index = 0\n",
    "    max_value = 0.0\n",
    "    for j in range(0, len(y_predict[0])):\n",
    "        if y_predict[i][j] > max_value:\n",
    "            max_value = y_predict[i][j]\n",
    "            max_index = j\n",
    "\n",
    "    y_predict_top1.append(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_emoji = pd.Series(y_predict_top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggested_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ex['suggested_emoji'] = suggested_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                  text  \\\n",
      "0                             when thieves broke into my house at night and held my wife and me on gun point for at least ten minutes and took away a lot  of property   \n",
      "1                                       International sports events won by myfavourite national team or playerbrings me joy when india won the world cup cricket match   \n",
      "2  I was sitting in a restaurant with friends they asked me something which they thoughtI should know Actually I know it but atthat time I was not able to remember it   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                   suggested_words  \\\n",
      "0  [yeah, lot, sun, beautiful, wonder, thank, well, say, hi, move, take, change, sing, page, dress, house, mean, kid, btw, congrats, life, birthday, break, crazy, iphone, com, www, minute, afternoon, tweeterfollow, night, haha, welcome, good, see, hey, meet, fall, cat, chat, leave, away, far, link, catch]   \n",
      "1                                                                                     [year, run, give, mind, ago, win, train, place, plan, probably, guy, miss, goodnight, aww, xd, site, soo, bday, account, record, day, today, update, good, beautiful, ur, glad, ha, sunday, busy, lol, happy, talk, tea, im]   \n",
      "2                                                                                                                [know, let, god, idea, happen, old, sorry, hear, remember, lmao, win, train, place, plan, probably, read, hard, sit, drive, lady, sure, summer, finish, ask, fine, need, vip, trip, story, cover]   \n",
      "\n",
      "   suggested_emoji  \n",
      "0                1  \n",
      "1                0  \n",
      "2                6  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 500):\n",
    "    print(d_ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
