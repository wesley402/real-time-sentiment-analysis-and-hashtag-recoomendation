{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "# refer: https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/ilab/users/kc1026/Documents/cs543/sentiment140_clean.csv\", sep=',', header=0)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(index=str, columns={\"Unnamed: 0\": \"index\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "stopWords = set(STOPWORDS)\n",
    "lmtzr = nltk.WordNetLemmatizer().lemmatize\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def normalize_text(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    cleaned_token_text = []\n",
    "    for tt in token_text:\n",
    "        if tt in stopWords or tt == '' or len(tt) < 2: continue    \n",
    "        cleaned_token_text.append(tt)\n",
    "    \n",
    "    word_pos = nltk.pos_tag(cleaned_token_text)\n",
    "    lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n",
    "    \n",
    "    return [x.lower() for x in lemm_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['lol']\n"
     ]
    }
   ],
   "source": [
    "# doc_sample='lol'\n",
    "# print('original document: ')\n",
    "\n",
    "# print('\\n\\n tokenized and lemmatized document: ')\n",
    "# print(normalize_text(doc_sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df['text'].map(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 awww\n",
      "1 bummer\n",
      "2 carr\n",
      "3 david\n",
      "4 day\n",
      "5 get\n",
      "6 shoulda\n",
      "7 blah\n",
      "8 facebook\n",
      "9 result\n",
      "10 school\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "# count = 0\n",
    "# for k, v in dictionary.iteritems():\n",
    "#     print(k, v)\n",
    "#     count += 1\n",
    "#     if count > 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(keep_n=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 2), (70, 1), (161, 1), (198, 1), (199, 1), (258, 1), (375, 1), (1722, 1)]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=75, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.340*\"happy\" + 0.128*\"leave\" + 0.099*\"hahaha\" + 0.073*\"stay\" + 0.039*\"tea\" + 0.032*\"hmm\" + 0.030*\"bday\" + 0.022*\"club\" + 0.021*\"jb\" + 0.020*\"shout\"\n",
      "Topic: 1 \n",
      "Words: 0.295*\"hope\" + 0.144*\"soon\" + 0.142*\"well\" + 0.091*\"th\" + 0.058*\"friday\" + 0.029*\"good\" + 0.028*\"ll\" + 0.028*\"go\" + 0.022*\"thursday\" + 0.021*\"mention\"\n",
      "Topic: 2 \n",
      "Words: 0.254*\"great\" + 0.223*\"look\" + 0.080*\"birthday\" + 0.048*\"kid\" + 0.045*\"forward\" + 0.041*\"like\" + 0.029*\"super\" + 0.027*\"ice\" + 0.026*\"st\" + 0.020*\"cream\"\n",
      "Topic: 3 \n",
      "Words: 0.162*\"glad\" + 0.148*\"www\" + 0.118*\"later\" + 0.097*\"hot\" + 0.083*\"place\" + 0.067*\"amazing\" + 0.057*\"hate\" + 0.026*\"like\" + 0.020*\"ball\" + 0.019*\"go\"\n",
      "Topic: 4 \n",
      "Words: 0.355*\"night\" + 0.177*\"tonight\" + 0.140*\"movie\" + 0.066*\"late\" + 0.034*\"go\" + 0.020*\"uk\" + 0.016*\"bar\" + 0.015*\"complete\" + 0.014*\"def\" + 0.012*\"time\"\n",
      "Topic: 5 \n",
      "Words: 0.141*\"idea\" + 0.131*\"totally\" + 0.074*\"worth\" + 0.072*\"gym\" + 0.068*\"pool\" + 0.061*\"soooo\" + 0.050*\"yo\" + 0.042*\"mad\" + 0.037*\"good\" + 0.020*\"soul\"\n",
      "Topic: 6 \n",
      "Words: 0.183*\"game\" + 0.058*\"nap\" + 0.055*\"july\" + 0.052*\"church\" + 0.048*\"wed\" + 0.043*\"couldn\" + 0.038*\"bitch\" + 0.036*\"shot\" + 0.034*\"paper\" + 0.030*\"topic\"\n",
      "Topic: 7 \n",
      "Words: 0.114*\"actually\" + 0.083*\"hehe\" + 0.082*\"vote\" + 0.061*\"happen\" + 0.057*\"cause\" + 0.051*\"definitely\" + 0.050*\"person\" + 0.044*\"season\" + 0.029*\"project\" + 0.027*\"hahah\"\n",
      "Topic: 8 \n",
      "Words: 0.234*\"want\" + 0.185*\"yes\" + 0.128*\"play\" + 0.124*\"enjoy\" + 0.028*\"support\" + 0.023*\"excited\" + 0.020*\"didnt\" + 0.019*\"fantastic\" + 0.018*\"mum\" + 0.017*\"huge\"\n",
      "Topic: 9 \n",
      "Words: 0.263*\"feel\" + 0.166*\"listen\" + 0.094*\"like\" + 0.054*\"good\" + 0.046*\"isn\" + 0.034*\"holiday\" + 0.032*\"one\" + 0.032*\"sick\" + 0.031*\"yup\" + 0.024*\"bad\"\n",
      "Topic: 10 \n",
      "Words: 0.112*\"reply\" + 0.080*\"close\" + 0.079*\"cat\" + 0.077*\"ticket\" + 0.065*\"sunshine\" + 0.056*\"mac\" + 0.043*\"aren\" + 0.043*\"french\" + 0.032*\"fave\" + 0.028*\"hows\"\n",
      "Topic: 11 \n",
      "Words: 0.188*\"add\" + 0.156*\"finish\" + 0.082*\"hopefully\" + 0.074*\"tweeterfollow\" + 0.064*\"sooo\" + 0.061*\"eye\" + 0.052*\"hell\" + 0.034*\"tweeteradder\" + 0.029*\"reading\" + 0.025*\"background\"\n",
      "Topic: 12 \n",
      "Words: 0.090*\"thx\" + 0.082*\"page\" + 0.081*\"awww\" + 0.080*\"yea\" + 0.080*\"point\" + 0.068*\"move\" + 0.045*\"mmm\" + 0.040*\"tan\" + 0.039*\"ohh\" + 0.034*\"jus\"\n",
      "Topic: 13 \n",
      "Words: 0.263*\"wait\" + 0.102*\"year\" + 0.093*\"maybe\" + 0.078*\"baby\" + 0.056*\"ask\" + 0.037*\"open\" + 0.024*\"come\" + 0.021*\"ll\" + 0.019*\"sort\" + 0.017*\"wine\"\n",
      "Topic: 14 \n",
      "Words: 0.468*\"think\" + 0.103*\"wow\" + 0.042*\"afternoon\" + 0.041*\"dude\" + 0.034*\"myspace\" + 0.023*\"good\" + 0.022*\"like\" + 0.018*\"ll\" + 0.017*\"addict\" + 0.013*\"go\"\n",
      "Topic: 15 \n",
      "Words: 0.167*\"beach\" + 0.152*\"kind\" + 0.110*\"test\" + 0.076*\"gorgeous\" + 0.049*\"draw\" + 0.041*\"havent\" + 0.034*\"shut\" + 0.033*\"grade\" + 0.029*\"artist\" + 0.029*\"go\"\n",
      "Topic: 16 \n",
      "Words: 0.117*\"okay\" + 0.087*\"far\" + 0.078*\"mind\" + 0.068*\"fuck\" + 0.057*\"soo\" + 0.051*\"fly\" + 0.049*\"red\" + 0.048*\"tour\" + 0.041*\"aw\" + 0.034*\"interest\"\n",
      "Topic: 17 \n",
      "Words: 0.257*\"na\" + 0.236*\"gon\" + 0.162*\"weekend\" + 0.056*\"lady\" + 0.036*\"luv\" + 0.029*\"touch\" + 0.022*\"taylor\" + 0.020*\"tweeps\" + 0.020*\"good\" + 0.013*\"detail\"\n",
      "Topic: 18 \n",
      "Words: 0.129*\"ha\" + 0.093*\"remember\" + 0.075*\"damn\" + 0.069*\"saturday\" + 0.064*\"green\" + 0.045*\"shall\" + 0.043*\"white\" + 0.032*\"taste\" + 0.032*\"roll\" + 0.027*\"medium\"\n",
      "Topic: 19 \n",
      "Words: 0.198*\"thank\" + 0.165*\"yeah\" + 0.126*\"com\" + 0.075*\"mean\" + 0.068*\"free\" + 0.038*\"dance\" + 0.036*\"dad\" + 0.031*\"la\" + 0.024*\"oh\" + 0.021*\"website\"\n",
      "Topic: 20 \n",
      "Words: 0.212*\"lot\" + 0.144*\"party\" + 0.094*\"hang\" + 0.077*\"ill\" + 0.075*\"trip\" + 0.047*\"em\" + 0.031*\"ooh\" + 0.022*\"today\" + 0.021*\"heh\" + 0.020*\"father\"\n",
      "Topic: 21 \n",
      "Words: 0.179*\"little\" + 0.153*\"read\" + 0.118*\"beautiful\" + 0.104*\"book\" + 0.036*\"meeting\" + 0.036*\"chill\" + 0.032*\"interview\" + 0.031*\"kick\" + 0.029*\"swim\" + 0.020*\"ahead\"\n",
      "Topic: 22 \n",
      "Words: 0.509*\"watch\" + 0.151*\"ready\" + 0.063*\"wonderful\" + 0.051*\"kinda\" + 0.030*\"episode\" + 0.022*\"interesting\" + 0.020*\"dvd\" + 0.014*\"hoo\" + 0.013*\"time\" + 0.012*\"peter\"\n",
      "Topic: 23 \n",
      "Words: 0.246*\"make\" + 0.162*\"funny\" + 0.062*\"hand\" + 0.053*\"cold\" + 0.051*\"download\" + 0.050*\"cook\" + 0.046*\"sad\" + 0.042*\"hold\" + 0.027*\"men\" + 0.024*\"idk\"\n",
      "Topic: 24 \n",
      "Words: 0.469*\"thanks\" + 0.093*\"pay\" + 0.063*\"lunch\" + 0.049*\"class\" + 0.029*\"mr\" + 0.027*\"til\" + 0.024*\"english\" + 0.023*\"da\" + 0.019*\"hurt\" + 0.017*\"bro\"\n",
      "Topic: 25 \n",
      "Words: 0.432*\"new\" + 0.111*\"ya\" + 0.072*\"phone\" + 0.062*\"stop\" + 0.043*\"iphone\" + 0.043*\"ah\" + 0.019*\"sexy\" + 0.018*\"wouldn\" + 0.016*\"mcfly\" + 0.016*\"web\"\n",
      "Topic: 26 \n",
      "Words: 0.489*\"lol\" + 0.108*\"eat\" + 0.059*\"hello\" + 0.033*\"pm\" + 0.030*\"list\" + 0.029*\"june\" + 0.025*\"moment\" + 0.025*\"lil\" + 0.014*\"hahahaha\" + 0.014*\"like\"\n",
      "Topic: 27 \n",
      "Words: 0.279*\"follow\" + 0.067*\"weather\" + 0.061*\"lmao\" + 0.050*\"heart\" + 0.046*\"high\" + 0.041*\"proud\" + 0.040*\"bet\" + 0.038*\"as\" + 0.031*\"people\" + 0.026*\"case\"\n",
      "Topic: 28 \n",
      "Words: 0.320*\"fun\" + 0.127*\"have\" + 0.038*\"dear\" + 0.034*\"time\" + 0.031*\"spend\" + 0.029*\"woman\" + 0.025*\"die\" + 0.023*\"business\" + 0.021*\"tired\" + 0.019*\"nope\"\n",
      "Topic: 29 \n",
      "Words: 0.190*\"cute\" + 0.187*\"live\" + 0.148*\"job\" + 0.058*\"goin\" + 0.052*\"ppl\" + 0.041*\"yum\" + 0.035*\"come\" + 0.026*\"good\" + 0.025*\"apparently\" + 0.024*\"fabulous\"\n",
      "Topic: 30 \n",
      "Words: 0.176*\"sleep\" + 0.108*\"excite\" + 0.099*\"pic\" + 0.066*\"fan\" + 0.055*\"early\" + 0.054*\"xx\" + 0.052*\"wake\" + 0.032*\"shit\" + 0.030*\"yep\" + 0.028*\"reason\"\n",
      "Topic: 31 \n",
      "Words: 0.286*\"week\" + 0.247*\"start\" + 0.137*\"train\" + 0.040*\"film\" + 0.028*\"college\" + 0.021*\"boo\" + 0.020*\"go\" + 0.020*\"american\" + 0.018*\"mornin\" + 0.018*\"epic\"\n",
      "Topic: 32 \n",
      "Words: 0.741*\"love\" + 0.018*\"mail\" + 0.017*\"speak\" + 0.017*\"shoot\" + 0.017*\"ive\" + 0.016*\"plus\" + 0.015*\"random\" + 0.014*\"single\" + 0.014*\"track\" + 0.014*\"expect\"\n",
      "Topic: 33 \n",
      "Words: 0.273*\"hey\" + 0.127*\"welcome\" + 0.066*\"course\" + 0.062*\"tv\" + 0.049*\"nite\" + 0.036*\"black\" + 0.035*\"park\" + 0.028*\"kiss\" + 0.025*\"small\" + 0.023*\"road\"\n",
      "Topic: 34 \n",
      "Words: 0.236*\"awesome\" + 0.184*\"cool\" + 0.149*\"ok\" + 0.028*\"weird\" + 0.026*\"instead\" + 0.025*\"google\" + 0.024*\"power\" + 0.024*\"ipod\" + 0.020*\"peace\" + 0.019*\"guitar\"\n",
      "Topic: 35 \n",
      "Words: 0.292*\"friend\" + 0.255*\"best\" + 0.099*\"vip\" + 0.062*\"line\" + 0.058*\"shop\" + 0.033*\"fast\" + 0.028*\"exactly\" + 0.022*\"go\" + 0.018*\"august\" + 0.013*\"time\"\n",
      "Topic: 36 \n",
      "Words: 0.163*\"sweet\" + 0.117*\"monday\" + 0.083*\"room\" + 0.070*\"dog\" + 0.065*\"set\" + 0.057*\"second\" + 0.042*\"xoxo\" + 0.034*\"ahhh\" + 0.031*\"realize\" + 0.030*\"woohoo\"\n",
      "Topic: 37 \n",
      "Words: 0.491*\"work\" + 0.183*\"guy\" + 0.045*\"laugh\" + 0.033*\"youtube\" + 0.031*\"appreciate\" + 0.025*\"chance\" + 0.021*\"cup\" + 0.021*\"today\" + 0.020*\"time\" + 0.020*\"pizza\"\n",
      "Topic: 38 \n",
      "Words: 0.434*\"know\" + 0.084*\"dont\" + 0.046*\"turn\" + 0.040*\"site\" + 0.036*\"problem\" + 0.025*\"like\" + 0.019*\"ll\" + 0.018*\"nick\" + 0.018*\"stupid\" + 0.015*\"good\"\n",
      "Topic: 39 \n",
      "Words: 0.136*\"video\" + 0.093*\"hard\" + 0.061*\"easy\" + 0.057*\"sing\" + 0.056*\"wear\" + 0.044*\"fix\" + 0.044*\"shopping\" + 0.038*\"beat\" + 0.036*\"shoe\" + 0.035*\"different\"\n",
      "Topic: 40 \n",
      "Words: 0.518*\"miss\" + 0.069*\"jealous\" + 0.066*\"apple\" + 0.050*\"homework\" + 0.048*\"earlier\" + 0.029*\"wing\" + 0.027*\"quality\" + 0.020*\"go\" + 0.017*\"home\" + 0.017*\"time\"\n",
      "Topic: 41 \n",
      "Words: 0.725*\"day\" + 0.063*\"today\" + 0.052*\"good\" + 0.050*\"sun\" + 0.018*\"beer\" + 0.018*\"go\" + 0.012*\"yesterday\" + 0.010*\"flower\" + 0.008*\"shin\" + 0.007*\"shine\"\n",
      "Topic: 42 \n",
      "Words: 0.176*\"buy\" + 0.129*\"wan\" + 0.126*\"na\" + 0.118*\"rock\" + 0.071*\"tho\" + 0.062*\"money\" + 0.041*\"min\" + 0.028*\"pink\" + 0.025*\"mother\" + 0.024*\"dark\"\n",
      "Topic: 43 \n",
      "Words: 0.147*\"take\" + 0.116*\"thats\" + 0.084*\"away\" + 0.080*\"exam\" + 0.075*\"till\" + 0.065*\"minute\" + 0.065*\"probably\" + 0.040*\"cake\" + 0.030*\"alright\" + 0.027*\"sell\"\n",
      "Topic: 44 \n",
      "Words: 0.214*\"pretty\" + 0.097*\"drink\" + 0.082*\"fine\" + 0.054*\"good\" + 0.053*\"heard\" + 0.045*\"water\" + 0.039*\"mood\" + 0.032*\"david\" + 0.026*\"country\" + 0.023*\"studio\"\n",
      "Topic: 45 \n",
      "Words: 0.572*\"get\" + 0.059*\"ta\" + 0.043*\"bring\" + 0.039*\"home\" + 0.031*\"ve\" + 0.026*\"chocolate\" + 0.022*\"today\" + 0.021*\"short\" + 0.020*\"feeling\" + 0.019*\"moon\"\n",
      "Topic: 46 \n",
      "Words: 0.341*\"nice\" + 0.227*\"let\" + 0.067*\"bore\" + 0.066*\"walk\" + 0.032*\"blue\" + 0.024*\"forever\" + 0.020*\"wont\" + 0.019*\"silly\" + 0.017*\"space\" + 0.014*\"brown\"\n",
      "Topic: 47 \n",
      "Words: 0.163*\"long\" + 0.129*\"head\" + 0.057*\"time\" + 0.053*\"pick\" + 0.047*\"cd\" + 0.046*\"order\" + 0.044*\"office\" + 0.043*\"band\" + 0.029*\"kill\" + 0.024*\"blood\"\n",
      "Topic: 48 \n",
      "Words: 0.177*\"win\" + 0.133*\"world\" + 0.088*\"end\" + 0.068*\"photo\" + 0.048*\"perfect\" + 0.045*\"relax\" + 0.037*\"shirt\" + 0.035*\"seriously\" + 0.030*\"tom\" + 0.029*\"cover\"\n",
      "Topic: 49 \n",
      "Words: 0.463*\"im\" + 0.205*\"girl\" + 0.067*\"break\" + 0.033*\"go\" + 0.027*\"chillin\" + 0.024*\"company\" + 0.018*\"itunes\" + 0.017*\"bird\" + 0.015*\"prepare\" + 0.015*\"good\"\n",
      "Topic: 50 \n",
      "Words: 0.228*\"finally\" + 0.081*\"care\" + 0.077*\"breakfast\" + 0.072*\"join\" + 0.054*\"bless\" + 0.048*\"london\" + 0.034*\"note\" + 0.028*\"land\" + 0.026*\"near\" + 0.024*\"home\"\n",
      "Topic: 51 \n",
      "Words: 0.314*\"twitter\" + 0.149*\"song\" + 0.068*\"update\" + 0.042*\"facebook\" + 0.034*\"lucky\" + 0.029*\"cuz\" + 0.027*\"lakers\" + 0.024*\"account\" + 0.024*\"nd\" + 0.022*\"camp\"\n",
      "Topic: 52 \n",
      "Words: 0.150*\"house\" + 0.144*\"music\" + 0.074*\"link\" + 0.066*\"share\" + 0.061*\"clean\" + 0.055*\"online\" + 0.045*\"tuesday\" + 0.034*\"vacation\" + 0.025*\"parent\" + 0.025*\"copy\"\n",
      "Topic: 53 \n",
      "Words: 0.161*\"hear\" + 0.155*\"sorry\" + 0.109*\"blog\" + 0.054*\"question\" + 0.048*\"answer\" + 0.037*\"wednesday\" + 0.036*\"type\" + 0.035*\"air\" + 0.029*\"good\" + 0.028*\"positive\"\n",
      "Topic: 54 \n",
      "Words: 0.195*\"good\" + 0.116*\"luck\" + 0.108*\"saw\" + 0.081*\"mom\" + 0.080*\"real\" + 0.077*\"dream\" + 0.046*\"city\" + 0.040*\"yummy\" + 0.040*\"dress\" + 0.019*\"magic\"\n",
      "Topic: 55 \n",
      "Words: 0.185*\"amaze\" + 0.123*\"family\" + 0.103*\"busy\" + 0.050*\"safe\" + 0.040*\"stick\" + 0.037*\"glass\" + 0.032*\"mile\" + 0.032*\"sale\" + 0.032*\"group\" + 0.021*\"chicago\"\n",
      "Topic: 56 \n",
      "Words: 0.137*\"sunday\" + 0.110*\"call\" + 0.102*\"drive\" + 0.100*\"aww\" + 0.052*\"twilight\" + 0.041*\"officially\" + 0.035*\"storm\" + 0.035*\"sleepy\" + 0.027*\"drunk\" + 0.026*\"starbucks\"\n",
      "Topic: 57 \n",
      "Words: 0.257*\"tell\" + 0.101*\"month\" + 0.084*\"catch\" + 0.079*\"shower\" + 0.052*\"cheer\" + 0.034*\"thanx\" + 0.028*\"time\" + 0.025*\"nyc\" + 0.024*\"spending\" + 0.023*\"rule\"\n",
      "Topic: 58 \n",
      "Words: 0.190*\"ur\" + 0.128*\"stuff\" + 0.054*\"concert\" + 0.048*\"couple\" + 0.044*\"sign\" + 0.036*\"decide\" + 0.036*\"internet\" + 0.035*\"light\" + 0.035*\"ride\" + 0.030*\"radio\"\n",
      "Topic: 59 \n",
      "Words: 0.136*\"old\" + 0.095*\"picture\" + 0.090*\"use\" + 0.090*\"coffee\" + 0.066*\"rest\" + 0.060*\"hit\" + 0.043*\"comment\" + 0.034*\"sims\" + 0.032*\"number\" + 0.030*\"squarespace\"\n",
      "Topic: 60 \n",
      "Words: 0.167*\"try\" + 0.127*\"sure\" + 0.072*\"see\" + 0.066*\"dinner\" + 0.064*\"write\" + 0.054*\"brother\" + 0.050*\"ll\" + 0.049*\"btw\" + 0.044*\"jonas\" + 0.040*\"haven\"\n",
      "Topic: 61 \n",
      "Words: 0.159*\"follower\" + 0.138*\"talk\" + 0.135*\"help\" + 0.085*\"goodnight\" + 0.052*\"worry\" + 0.052*\"wonder\" + 0.048*\"lose\" + 0.041*\"final\" + 0.034*\"team\" + 0.028*\"need\"\n",
      "Topic: 62 \n",
      "Words: 0.267*\"morning\" + 0.144*\"good\" + 0.123*\"bed\" + 0.058*\"bit\" + 0.058*\"go\" + 0.038*\"forget\" + 0.029*\"text\" + 0.028*\"star\" + 0.027*\"hop\" + 0.024*\"time\"\n",
      "Topic: 63 \n",
      "Words: 0.133*\"hair\" + 0.109*\"crazy\" + 0.058*\"store\" + 0.057*\"cut\" + 0.053*\"record\" + 0.042*\"age\" + 0.038*\"twit\" + 0.038*\"info\" + 0.031*\"p\" + 0.029*\"pain\"\n",
      "Topic: 64 \n",
      "Words: 0.306*\"way\" + 0.220*\"life\" + 0.089*\"change\" + 0.085*\"tire\" + 0.036*\"good\" + 0.024*\"service\" + 0.020*\"home\" + 0.017*\"lovin\" + 0.015*\"mama\" + 0.015*\"arm\"\n",
      "Topic: 65 \n",
      "Words: 0.326*\"right\" + 0.181*\"wish\" + 0.048*\"award\" + 0.046*\"xxx\" + 0.039*\"hangover\" + 0.032*\"count\" + 0.025*\"travel\" + 0.021*\"joy\" + 0.021*\"fair\" + 0.020*\"sir\"\n",
      "Topic: 66 \n",
      "Words: 0.110*\"car\" + 0.096*\"study\" + 0.070*\"miley\" + 0.070*\"wrong\" + 0.065*\"bye\" + 0.052*\"whats\" + 0.052*\"absolutely\" + 0.039*\"demi\" + 0.033*\"pleasure\" + 0.032*\"possible\"\n",
      "Topic: 67 \n",
      "Words: 0.275*\"haha\" + 0.123*\"yay\" + 0.081*\"hi\" + 0.062*\"meet\" + 0.057*\"omg\" + 0.047*\"smile\" + 0.033*\"xd\" + 0.033*\"believe\" + 0.026*\"date\" + 0.023*\"bout\"\n",
      "Topic: 68 \n",
      "Words: 0.227*\"tweet\" + 0.134*\"check\" + 0.090*\"god\" + 0.087*\"send\" + 0.068*\"plan\" + 0.039*\"hug\" + 0.033*\"message\" + 0.030*\"able\" + 0.028*\"ll\" + 0.022*\"woo\"\n",
      "Topic: 69 \n",
      "Words: 0.255*\"tomorrow\" + 0.143*\"school\" + 0.129*\"sound\" + 0.051*\"like\" + 0.037*\"good\" + 0.032*\"sunny\" + 0.032*\"go\" + 0.031*\"fall\" + 0.028*\"town\" + 0.027*\"woke\"\n",
      "Topic: 70 \n",
      "Words: 0.090*\"lovely\" + 0.081*\"album\" + 0.078*\"congrats\" + 0.062*\"visit\" + 0.059*\"sister\" + 0.055*\"news\" + 0.038*\"pack\" + 0.036*\"keep\" + 0.031*\"babe\" + 0.029*\"everybody\"\n",
      "Topic: 71 \n",
      "Words: 0.198*\"hour\" + 0.112*\"rain\" + 0.096*\"sit\" + 0.069*\"agree\" + 0.063*\"half\" + 0.061*\"outside\" + 0.039*\"awake\" + 0.035*\"warm\" + 0.025*\"excellent\" + 0.023*\"pas\"\n",
      "Topic: 72 \n",
      "Words: 0.150*\"big\" + 0.111*\"run\" + 0.084*\"word\" + 0.073*\"favorite\" + 0.061*\"face\" + 0.058*\"email\" + 0.048*\"story\" + 0.040*\"chat\" + 0.029*\"drop\" + 0.028*\"hilarious\"\n",
      "Topic: 73 \n",
      "Words: 0.185*\"say\" + 0.123*\"post\" + 0.086*\"boy\" + 0.082*\"true\" + 0.042*\"suck\" + 0.039*\"special\" + 0.032*\"choice\" + 0.027*\"si\" + 0.021*\"deserve\" + 0.018*\"like\"\n",
      "Topic: 74 \n",
      "Words: 0.167*\"summer\" + 0.090*\"food\" + 0.089*\"give\" + 0.068*\"learn\" + 0.059*\"ago\" + 0.042*\"ahh\" + 0.042*\"wasn\" + 0.039*\"dm\" + 0.032*\"eh\" + 0.029*\"deal\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.17581303417682648\t Topic: 0.434*\"know\" + 0.084*\"dont\" + 0.046*\"turn\" + 0.040*\"site\" + 0.036*\"problem\"\n",
      "Score: 0.09711853414773941\t Topic: 0.195*\"good\" + 0.116*\"luck\" + 0.108*\"saw\" + 0.081*\"mom\" + 0.080*\"real\"\n",
      "Score: 0.09648895263671875\t Topic: 0.292*\"friend\" + 0.255*\"best\" + 0.099*\"vip\" + 0.062*\"line\" + 0.058*\"shop\"\n",
      "Score: 0.09616363048553467\t Topic: 0.114*\"actually\" + 0.083*\"hehe\" + 0.082*\"vote\" + 0.061*\"happen\" + 0.057*\"cause\"\n",
      "Score: 0.09603578597307205\t Topic: 0.263*\"wait\" + 0.102*\"year\" + 0.093*\"maybe\" + 0.078*\"baby\" + 0.056*\"ask\"\n",
      "Score: 0.09409008175134659\t Topic: 0.468*\"think\" + 0.103*\"wow\" + 0.042*\"afternoon\" + 0.041*\"dude\" + 0.034*\"myspace\"\n",
      "Score: 0.09133269637823105\t Topic: 0.129*\"ha\" + 0.093*\"remember\" + 0.075*\"damn\" + 0.069*\"saturday\" + 0.064*\"green\"\n",
      "Score: 0.09084299206733704\t Topic: 0.198*\"hour\" + 0.112*\"rain\" + 0.096*\"sit\" + 0.069*\"agree\" + 0.063*\"half\"\n",
      "Score: 0.08878091722726822\t Topic: 0.227*\"tweet\" + 0.134*\"check\" + 0.090*\"god\" + 0.087*\"send\" + 0.068*\"plan\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = ' I was sitting in a restaurant with friends  They asked me something which they thought I should know  Actually I know it  but at that time I was not able to remember it  '\n",
    "bow_vector = dictionary.doc2bow(normalize_text(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
