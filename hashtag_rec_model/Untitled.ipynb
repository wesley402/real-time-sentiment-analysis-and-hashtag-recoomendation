{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = \"/ilab/users/kc1026/Documents/cs543/GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 175303 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 150 # max number of words in a comment to use\n",
    "\n",
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/ilab/users/kc1026/Documents/cs543/emoji.csv\", sep=',', header=0)\n",
    "train, test = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/lib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(\"[^a-zA-Z']\", ' ', x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub(\"[^a-zA-Z']\", ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When I was driving home after  several days of hard work  there was a motorist ahead of me who was driving at    km hour and refused  despite his low speeed to let me overtake '"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_training_words = [word for token in train[\"token\"] for word in token]\n",
    "# training_sentence_lengths = [len(token) for token in train[\"token\"]]\n",
    "# TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "# print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "# print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8543 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(train[\"text\"].tolist())\n",
    "x_training_sequences = tokenizer.texts_to_sequences(train[\"text\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "# set max length of sequences, now all data has the same length of 300\n",
    "train_cnn_data = pad_sequences(x_training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "num_words = len(train_word_index) + 1\n",
    "train_embedding_weights = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    if word in word2vec:\n",
    "        train_embedding_weights[index,:] = word2vec[word]     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   11,   15,  170],\n",
       "       [   0,    0,    0, ...,    1,   11,  576],\n",
       "       [   0,    0,    0, ..., 4368,  617,  385],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4,   81,   59],\n",
       "       [   0,    0,    0, ..., 8543,   12,  365],\n",
       "       [   0,    0,    0, ..., 2355,    5,   34]], dtype=int32)"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test[\"text\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = test_cnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_train = np_utils.to_categorical(train['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index=7, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.25)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.25)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    # Finally, we feed the output into a Sigmoid layer.\n",
    "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
    "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "    preds = Dense(labels_index, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_44 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_52 (Embedding)        (None, 150, 300)     2563200     input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_172 (Conv1D)             (None, 148, 128)     115328      embedding_52[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_173 (Conv1D)             (None, 147, 128)     153728      embedding_52[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_174 (Conv1D)             (None, 146, 128)     192128      embedding_52[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_172 (MaxPooling1D (None, 49, 128)      0           conv1d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_173 (MaxPooling1D (None, 49, 128)      0           conv1d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_174 (MaxPooling1D (None, 48, 128)      0           conv1d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 146, 128)     0           max_pooling1d_172[0][0]          \n",
      "                                                                 max_pooling1d_173[0][0]          \n",
      "                                                                 max_pooling1d_174[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 146, 128)     0           concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_51 (Flatten)            (None, 18688)        0           dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 100)          1868900     flatten_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 100)          0           dense_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_102 (Dense)               (None, 7)            707         dropout_86[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,893,991\n",
      "Trainable params: 2,330,791\n",
      "Non-trainable params: 2,563,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, num_words, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6058 samples, validate on 674 samples\n",
      "Epoch 1/20\n",
      "6058/6058 [==============================] - 14s 2ms/step - loss: 2.1983 - acc: 0.1651 - val_loss: 1.9304 - val_acc: 0.1929\n",
      "Epoch 2/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.9465 - acc: 0.1722 - val_loss: 1.9363 - val_acc: 0.1706\n",
      "Epoch 3/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.9254 - acc: 0.1892 - val_loss: 1.9073 - val_acc: 0.2033\n",
      "Epoch 4/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.9148 - acc: 0.1961 - val_loss: 1.8836 - val_acc: 0.2270\n",
      "Epoch 5/20\n",
      "6058/6058 [==============================] - 8s 1ms/step - loss: 1.8906 - acc: 0.2202 - val_loss: 1.8413 - val_acc: 0.2804\n",
      "Epoch 6/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.8792 - acc: 0.2230 - val_loss: 1.8183 - val_acc: 0.2596\n",
      "Epoch 7/20\n",
      "6058/6058 [==============================] - 8s 1ms/step - loss: 1.8528 - acc: 0.2364 - val_loss: 1.7410 - val_acc: 0.2878\n",
      "Epoch 8/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.8430 - acc: 0.2463 - val_loss: 1.7875 - val_acc: 0.2552\n",
      "Epoch 9/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.8201 - acc: 0.2616 - val_loss: 1.7902 - val_acc: 0.2611\n",
      "Epoch 10/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.8040 - acc: 0.2517 - val_loss: 1.8221 - val_acc: 0.2552\n",
      "Epoch 11/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.7893 - acc: 0.2710 - val_loss: 1.7197 - val_acc: 0.3042\n",
      "Epoch 12/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.7984 - acc: 0.2727 - val_loss: 2.1073 - val_acc: 0.2315\n",
      "Epoch 13/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.7722 - acc: 0.2776 - val_loss: 1.7400 - val_acc: 0.3071\n",
      "Epoch 14/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.7737 - acc: 0.2775 - val_loss: 1.7489 - val_acc: 0.2849\n",
      "Epoch 15/20\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 1.7480 - acc: 0.2877 - val_loss: 1.7921 - val_acc: 0.2760\n",
      "Epoch 00015: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748/748 [==============================] - 3s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(x_test, batch_size=64, verbose=1)\n",
    "y_actual = test['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_top1 = []\n",
    "for i in range(0, len(y_predict)):\n",
    "    max_index = 0\n",
    "    max_value = 0.0\n",
    "    for j in range(0, len(y_predict[0])):\n",
    "        if y_predict[i][j] > max_value:\n",
    "            max_value = y_predict[i][j]\n",
    "            max_index = j\n",
    "\n",
    "    y_predict_top1.append(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_actual, y_predict_top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2660427807486631"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'UNK'), ('Fulton', 'UNK'), ('County', 'UNK'), ...]"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.brown.tagged_words(tagset='CONJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
