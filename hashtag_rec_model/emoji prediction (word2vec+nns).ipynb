{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec_path = \"/ilab/users/kc1026/Documents/cs543/GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tr = []\n",
    "score_ts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 175303 \n",
    "MAX_SEQUENCE_LENGTH = 150 # max number of words in a comment to use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     7480\n",
       "label    7480\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/ilab/users/kc1026/Documents/cs543/emoji.csv\", sep=',', header=0)\n",
    "train, test = train_test_split(df, test_size=0.1)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/lib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(\"[^a-zA-Z']\", ' ', x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub(\"[^a-zA-Z']\", ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_training_words = [word for token in train[\"token\"] for word in token]\n",
    "# training_sentence_lengths = [len(token) for token in train[\"token\"]]\n",
    "# TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "# print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "# print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8522 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(train[\"text\"].tolist())\n",
    "x_training_sequences = tokenizer.texts_to_sequences(train[\"text\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "# set max length of sequences, now all data has the same length of 300\n",
    "train_cnn_data = pad_sequences(x_training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "num_words = len(train_word_index) + 1\n",
    "train_embedding_weights = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    if word in word2vec:\n",
    "        train_embedding_weights[index,:] = word2vec[word]     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test[\"text\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = test_cnn_data\n",
    "y_test = np_utils.to_categorical(test['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_train = np_utils.to_categorical(train['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 300)          2556900   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 45000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               11520256  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 14,078,955\n",
      "Trainable params: 14,078,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "#training params\n",
    "batch_size = 128 \n",
    "num_epochs = 6 \n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6058 samples, validate on 674 samples\n",
      "Epoch 1/6\n",
      "6058/6058 [==============================] - 9s 1ms/step - loss: 2.6474 - acc: 0.1741 - val_loss: 1.9107 - val_acc: 0.2433\n",
      "Epoch 2/6\n",
      "6058/6058 [==============================] - 6s 1ms/step - loss: 1.5753 - acc: 0.4473 - val_loss: 1.5250 - val_acc: 0.4392\n",
      "Epoch 3/6\n",
      "6058/6058 [==============================] - 6s 971us/step - loss: 0.9024 - acc: 0.7309 - val_loss: 1.4107 - val_acc: 0.4970\n",
      "Epoch 4/6\n",
      "6058/6058 [==============================] - 6s 958us/step - loss: 0.4229 - acc: 0.8919 - val_loss: 1.3534 - val_acc: 0.5401\n",
      "Epoch 5/6\n",
      "6058/6058 [==============================] - 6s 1ms/step - loss: 0.1865 - acc: 0.9581 - val_loss: 1.5176 - val_acc: 0.5178\n",
      "Epoch 6/6\n",
      "6058/6058 [==============================] - 7s 1ms/step - loss: 0.0815 - acc: 0.9835 - val_loss: 3.6080 - val_acc: 0.3234\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tr.append(model.evaluate(x_train, y_train, verbose=0))\n",
    "score_ts.append(model.evaluate(x_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9412121892471222, 0.7011289364407619]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.874503352425315, 0.3155080213903743]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([2, 4, 6, 8, 10], np.array(score_tr)[:,0], label=\"Train\")\n",
    "plt.plot([2, 4, 6, 8, 10], np.array(score_ts)[:,0], label=\"Test\")\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title(\"Loss with Diff Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([2, 4, 6, 8, 10], np.array(score_tr)[:,1], label=\"Train\")\n",
    "plt.plot([2, 4, 6, 8, 10], np.array(score_ts)[:,1], label=\"Test\")\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.title(\"Accuracy with Diff Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict = model.predict(x_test, batch_size=64, verbose=1)\n",
    "# y_actual = test['label'].tolist()\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# y_predict_top1 = []\n",
    "# for i in range(0, len(y_predict)):\n",
    "#     max_index = 0\n",
    "#     max_value = 0.0\n",
    "#     for j in range(0, len(y_predict[0])):\n",
    "#         if y_predict[i][j] > max_value:\n",
    "#             max_value = y_predict[i][j]\n",
    "#             max_index = j\n",
    "\n",
    "#     y_predict_top1.append(max_index)\n",
    "\n",
    "# accuracy = accuracy_score(y_actual, y_predict_top1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict = model.predict(x_test, batch_size=64, verbose=1)\n",
    "# y_actual = test['label'].tolist()\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# y_predict_top1 = []\n",
    "# for i in range(0, len(y_predict)):\n",
    "#     max_index = 0\n",
    "#     max_value = 0.0\n",
    "#     for j in range(0, len(y_predict[0])):\n",
    "#         if y_predict[i][j] > max_value:\n",
    "#             max_value = y_predict[i][j]\n",
    "#             max_index = j\n",
    "\n",
    "#     y_predict_top1.append(max_index)\n",
    "\n",
    "# accuracy = accuracy_score(y_actual, y_predict_top1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
