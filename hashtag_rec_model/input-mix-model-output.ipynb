{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import re\n",
    "stopWords = set(STOPWORDS)\n",
    "lmtzr = nltk.WordNetLemmatizer().lemmatize\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def normalize_text(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    cleaned_token_text = []\n",
    "    for tt in token_text:\n",
    "        if tt in stopWords or tt == '' or len(tt) < 2: continue    \n",
    "        cleaned_token_text.append(tt)\n",
    "    \n",
    "    word_pos = nltk.pos_tag(cleaned_token_text)\n",
    "    lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n",
    "    \n",
    "    return [x.lower() for x in lemm_words]\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = re.sub(\"[^a-zA-Z']\", ' ', text)\n",
    "#     cleaned_txt = ''\n",
    "#     for word in normalize_text(text):\n",
    "#         cleaned_txt += (word + ' ')\n",
    "#     return cleaned_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': ['when thieves broke into my house at night, and held my wife and me on gun point for at least ten minutes and took away a lot of property.',\n",
    "                 'International sports events won by my favourite national team or player brings me joy when india won the world cup cricket match.',\n",
    "                 'I was sitting in a restaurant with friends. They asked me something which they thought I should know. Actually I know it but at that time I was not able to remember it.']}\n",
    "d_ex = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                       text\n",
      "0                                 when thieves broke into my house at night, and held my wife and me on gun point for at least ten minutes and took away a lot of property.\n",
      "1                                         International sports events won by my favourite national team or player brings me joy when india won the world cup cricket match.\n",
      "2  I was sitting in a restaurant with friends. They asked me something which they thought I should know. Actually I know it but at that time I was not able to remember it.\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 500):\n",
    "    print(d_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_ex['cleaned_text'] = d_ex['text'].apply(lambda x: clean_text(x))\n",
    "# with pd.option_context('display.max_colwidth', 500):\n",
    "#     print(d_ex['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel.load('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load('lda_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug_words_list = []\n",
    "for text in d_ex['text']:\n",
    "    doc_bow = dictionary.doc2bow(normalize_text(text))\n",
    "    doc_lda = lda_model[doc_bow]\n",
    "    temp = []\n",
    "    for index, score in sorted(doc_lda, key=lambda tup: -1*tup[1]):\n",
    "        for word, score in lda_model.show_topic(index, 4):\n",
    "            temp.append(word)\n",
    "    sug_words_list.append(temp)\n",
    "\n",
    "sug_words = pd.Series(sug_words_list)\n",
    "d_ex['suggested_words'] = sug_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = load_model('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "3/3 [==============================] - 0s 108ms/step\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150\n",
    "y_predict = cnn_model.predict(pad_sequences(tokenizer.texts_to_sequences(d_ex['text'].tolist()), maxlen=MAX_SEQUENCE_LENGTH)\n",
    ", batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_top1 = []\n",
    "for i in range(0, len(y_predict)):\n",
    "    max_index = 0\n",
    "    max_value = 0.0\n",
    "    for j in range(0, len(y_predict[0])):\n",
    "        if y_predict[i][j] > max_value:\n",
    "            max_value = y_predict[i][j]\n",
    "            max_index = j\n",
    "\n",
    "    y_predict_top1.append(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_emoji = pd.Series(y_predict_top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ex['suggested_emoji'] = suggested_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                       text  \\\n",
      "0                                 when thieves broke into my house at night, and held my wife and me on gun point for at least ten minutes and took away a lot of property.   \n",
      "1                                         International sports events won by my favourite national team or player brings me joy when india won the world cup cricket match.   \n",
      "2  I was sitting in a restaurant with friends. They asked me something which they thought I should know. Actually I know it but at that time I was not able to remember it.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                suggested_words  \\\n",
      "0                                                                  [take, rock, walk, ill, buy, stuff, busy, worry, away, person, soo, hug, house, sweet, pick, award, bad, welcome, hehe, dream, add, later, picture, plan, wait, minute, yea, nap, morning, good, night, bed]   \n",
      "1  [saw, mom, tea, lil, win, hang, set, half, sorry, game, hear, kid, year, old, luck, good, week, book, class, haven, man, omg, ha, fan, world, believe, blue, internet, cool, ur, post, update, twitter, finally, funny, real, lunch, bring, ah, site, miss, write, boy, far]   \n",
      "2                                                                                    [know, let, money, perfect, hey, idea, course, reply, meet, ask, second, keep, friend, best, th, lovely, movie, eat, see, god, follow, remember, link, english, way, sit, hopefully, room]   \n",
      "\n",
      "   suggested_emoji  \n",
      "0                1  \n",
      "1                0  \n",
      "2                5  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 500):\n",
    "    print(d_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
